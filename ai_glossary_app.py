import streamlit as st
import pandas as pd
from fuzzywuzzy import fuzz

# Yeni veri: Yapay zeka terimleri ve aÃ§Ä±klamalarÄ±
data = [
    {"English": "Artificial Intelligence", "Turkish": "Yapay Zeka", "Azerbaijani": "SÃ¼ni Ä°ntellekt", "Description": "Yapay zeka, makinelerin insanlar gibi dÃ¼ÅŸÃ¼nmesini saÄŸlayan bir alandÄ±r."},
    {"English": "Machine Learning", "Turkish": "Makine Ã–ÄŸrenmesi", "Azerbaijani": "MaÅŸÄ±n Ã–yrÉ™nmÉ™si", "Description": "Makine Ã¶ÄŸrenmesi, bilgisayarlarÄ±n verilerle Ã¶ÄŸrenme sÃ¼recidir."},
    {"English": "Deep Learning", "Turkish": "Derin Ã–ÄŸrenme", "Azerbaijani": "DÉ™rin Ã–yrÉ™nmÉ™", "Description": "Derin Ã¶ÄŸrenme, daha karmaÅŸÄ±k modelleme ve bÃ¼yÃ¼k veri ile Ã¶ÄŸrenme yÃ¶ntemidir."},
    {"English": "Neural Network", "Turkish": "Sinir AÄŸÄ±", "Azerbaijani": "Sinir ÅÉ™bÉ™kÉ™si", "Description": "Sinir aÄŸÄ±, insan beyninin Ã§alÄ±ÅŸma prensibinden ilham alÄ±narak tasarlanmÄ±ÅŸ bir makine Ã¶ÄŸrenmesi modelidir."},
    {"English": "Natural Language Processing", "Turkish": "DoÄŸal Dil Ä°ÅŸleme", "Azerbaijani": "TÉ™bii Dil EmalÄ±", "Description": "DoÄŸal dil iÅŸleme, makinelerin insan dilini anlamasÄ± ve iÅŸlemesi iÃ§in kullanÄ±lan bir alandÄ±r."},
    {"English": "Reinforcement Learning", "Turkish": "PekiÅŸtirmeli Ã–ÄŸrenme", "Azerbaijani": "MÃ¶hkÉ™mlÉ™ndirici Ã–yrÉ™nmÉ™", "Description": "PekiÅŸtirmeli Ã¶ÄŸrenme, ajanlarÄ±n bir ortamda Ã¶dÃ¼ller veya cezalar alarak karar vermeyi Ã¶ÄŸrenmesidir."},
    {"English": "Computer Vision", "Turkish": "Bilgisayarla GÃ¶rÃ¼", "Azerbaijani": "KompÃ¼ter GÃ¶rmÉ™si", "Description": "Bilgisayarla gÃ¶rÃ¼, makinelerin gÃ¶rÃ¼ntÃ¼leri anlamasÄ± ve iÅŸlemesiyle ilgili bir alandÄ±r."},
    {"English": "Transformer", "Turkish": "DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼", "Azerbaijani": "Transformator", "Description": "Transformer, dil modellerinde kullanÄ±lan ve bÃ¼yÃ¼k veri Ã¼zerinde etkili olan bir derin Ã¶ÄŸrenme modelidir."},
    {"English": "Recurrent Neural Network", "Turkish": "Tekrarlayan Sinir AÄŸÄ±", "Azerbaijani": "TÉ™krarlanan Sinir ÅÉ™bÉ™kÉ™si", "Description": "Tekrarlayan sinir aÄŸlarÄ±, sÄ±ralÄ± veri iÅŸlemek iÃ§in kullanÄ±lan bir tÃ¼r sinir aÄŸÄ±dÄ±r."},
    {"English": "Convolutional Neural Network", "Turkish": "KonvolÃ¼syonel Sinir AÄŸÄ±", "Azerbaijani": "Konvolusyonal Sinir ÅÉ™bÉ™kÉ™si", "Description": "KonvolÃ¼syonel sinir aÄŸlarÄ±, Ã¶zellikle gÃ¶rÃ¼ntÃ¼ iÅŸleme iÃ§in yaygÄ±n olarak kullanÄ±lÄ±r."},
    {"English": "Generative Adversarial Networks", "Turkish": "Ãœretici KarÅŸÄ±t AÄŸlar", "Azerbaijani": "NÉ™sil ÆksÉ™ UÄŸurlu ÅÉ™bÉ™kÉ™lÉ™r", "Description": "Ãœretici karÅŸÄ±t aÄŸlar, gerÃ§ekÃ§i veriler Ã¼retmek iÃ§in kullanÄ±lan bir yapay zeka modelidir."},
    {"English": "Natural Language Generation", "Turkish": "DoÄŸal Dil Ãœretimi", "Azerbaijani": "TÉ™bii Dil Yaratma", "Description": "DoÄŸal dil Ã¼retimi, makinelerin anlamlÄ± metinler Ã¼retmesini saÄŸlayan bir alandÄ±r."},
    {"English": "AutoML", "Turkish": "Otomatik Makine Ã–ÄŸrenmesi", "Azerbaijani": "Avtomatik MaÅŸÄ±n Ã–yrÉ™nmÉ™si", "Description": "Otomatik makine Ã¶ÄŸrenmesi, makine Ã¶ÄŸrenmesi modellerini otomatik olarak oluÅŸturmak iÃ§in kullanÄ±lan bir tekniktir."},
]

df = pd.DataFrame(data)

st.title("ğŸ¤– Yapay Zeka Terimleri SÃ¶zlÃ¼ÄŸÃ¼ (TR / AZ)")

# KullanÄ±cÄ±nÄ±n arama yapacaÄŸÄ± metin giriÅŸi
search_term = st.text_input("ğŸ” Bir terim girin (Ä°ngilizce / TÃ¼rkÃ§e / Azerbaycanca):").strip().lower()

# Dil seÃ§imi
language = st.radio("Dil SeÃ§in:", ("English", "Turkish", "Azerbaijani"))

# Favori terimler listesi
if 'favorites' not in st.session_state:
    st.session_state.favorites = []

# En Ã§ok aranan terimler
if 'search_history' not in st.session_state:
    st.session_state.search_history = []

def search_glossary(term, language):
    if term == "":
        return df
    else:
        def is_match(row):
            return fuzz.partial_ratio(term, row[language].lower()) > 70
        return df[df.apply(is_match, axis=1)]

filtered_df = search_glossary(search_term, language)

# SonuÃ§larÄ± kullanÄ±cÄ±ya gÃ¶ster
st.write(f"### SonuÃ§lar ({len(filtered_df)} terim bulundu)")
for idx, row in filtered_df.iterrows():
    st.write(f"**{row['English']}** ({row['Turkish']} / {row['Azerbaijani']})")
    st.write(f"  *AÃ§Ä±klama:* {row['Description']}")
    if st.button(f"Favorilere Ekle: {row['English']}", key=f"fav_{row['English']}"):
        st.session_state.favorites.append(row['English'])
        st.success(f"{row['English']} favorilerinize eklendi!")

# Favori terimleri gÃ¶ster
st.write("### Favori Terimler")
if st.session_state.favorites:
    st.write(", ".join(st.session_state.favorites))
else:
    st.write("HenÃ¼z favori teriminiz yok.")

# En Ã§ok aranan terimleri gÃ¶ster
if len(st.session_state.search_history) > 0:
    st.write("### En Ã‡ok Aranan Terimler")
    st.write(", ".join(st.session_state.search_history))

# Arama geÃ§miÅŸini gÃ¼ncelle
if search_term and search_term not in st.session_state.search_history:
    st.session_state.search_history.append(search_term)

# Yeni terim ekleme formu
st.write("### Yeni Terim Ekle")
new_english = st.text_input("Ä°ngilizce Terim:", key="new_english")
new_turkish = st.text_input("TÃ¼rkÃ§e Terim:", key="new_turkish")
new_azerbaijani = st.text_input("Azerbaycan TÃ¼rkÃ§esi Terim:", key="new_azerbaijani")
new_description = st.text_area("AÃ§Ä±klama:", key="new_description")

# KullanÄ±cÄ± yeni terim eklerse
if st.button("Yeni Terim Ekle"):
    if new_english and new_turkish and new_azerbaijani and new_description:
        new_row = {
            "English": new_english,
            "Turkish": new_turkish,
            "Azerbaijani": new_azerbaijani,
            "Description": new_description
        }
        df = df.append(new_row, ignore_index=True)
        st.session_state.new_english = ""
        st.session_state.new_turkish = ""
        st.session_state.new_azerbaijani = ""
        st.session_state.new_description = ""
        st.success(f"Yeni terim baÅŸarÄ±yla eklendi: {new_english}")
    else:
        st.error("LÃ¼tfen tÃ¼m alanlarÄ± doldurduÄŸunuzdan emin olun.")
